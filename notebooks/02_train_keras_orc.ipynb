{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import itertools\n",
    "\n",
    "reader = easyocr.Reader(['en']) # this needs to run only once to load the model into memory\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "ANNOTATIONS_FILE = \"annotations.pkl\"\n",
    "IMAGE_PREFIX = \"../data/train/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_pickle(ANNOTATIONS_FILE)\n",
    "annotations = annotations[annotations['chart-type'] == 'vertical_bar']\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n",
    "IMAGE_SIZE = (128, 128)\n",
    "img_width, img_height = IMAGE_SIZE\n",
    "\n",
    "def extract_labels(row):\n",
    "    image = rgb2gray(skimage.io.imread(IMAGE_PREFIX + row['image']))\n",
    "    tick_labels = [text for text in row['text'] if text['role'] == 'tick_label']\n",
    "    labels = []\n",
    "    for tick_label in tick_labels:\n",
    "        polygon = tick_label['polygon']\n",
    "        x_min = min(polygon['x0'], polygon['x1'], polygon['x2'], polygon['x3'])\n",
    "        x_max = max(polygon['x0'], polygon['x1'], polygon['x2'], polygon['x3'])\n",
    "        y_min = min(polygon['y0'], polygon['y1'], polygon['y2'], polygon['y3'])\n",
    "        y_max = max(polygon['y0'], polygon['y1'], polygon['y2'], polygon['y3'])\n",
    "        sub_image = image[y_min:y_max, x_min:x_max]\n",
    "        output_image = (np.ones(shape=IMAGE_SIZE)*1).astype(float)\n",
    "        output_image[:len(sub_image),:len(sub_image[0])] = sub_image\n",
    "        labels.append((output_image, tick_label['text']))\n",
    "    return labels\n",
    "\n",
    "examples = list(itertools.chain.from_iterable(annotations.sample(100, random_state=123).apply(extract_labels, 1).values))\n",
    "\n",
    "print(np.array([example[0].shape for example in examples]).max(axis=0))\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples[:1]:\n",
    "    plt.imshow(example[0])\n",
    "    plt.title(example[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = zip(*examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set(char for label in train_y for char in label)\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "max_length = max([len(label) for label in train_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images found: \", len(train_y))\n",
    "print(\"Number of labels found: \", len(train_y))\n",
    "print(\"Number of unique labels found: \", len(set(train_y)))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n",
    "print(\"Max. label length: \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://keras.io/examples/vision/captcha_ocr/\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# Mapping characters to integers\n",
    "char_to_num = layers.StringLookup(\n",
    "    vocabulary=list(characters), mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(\n",
    "        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str):\n",
    "    encoded = char_to_num(tf.strings.unicode_split(text, input_encoding=\"UTF-8\"))\n",
    "    print(type(text), text)\n",
    "    padding = tf.zeros(max_length - tf.strings.length(text), tf.int64)\n",
    "    return tf.concat([encoded, padding], 0)\n",
    "\n",
    "encode_text(\"bye\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample(image, label):\n",
    "    print(label)\n",
    "    image = tf.reshape(image, (img_width, img_height, 1))\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = encode_text(label)\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((list(train_x), list(train_y)))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    # labels = batch[\"label\"]\n",
    "    #print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "def generator():\n",
    "    for x, y in zip(train_x, train_y):\n",
    "        yield {\"image\": x, \"label\": encode_text(y)}, 0\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
